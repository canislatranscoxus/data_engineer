Spark

1. read the event.json file and create a dataframe like this

| event_name | properties | colors | time_stamp |
|---|---|---|---|
"e01" | "A" | "A_red"   | "2023-01-15 05:00:00"
"e01" | "B" | "B_blue"  | "2023-01-15 05:00:00"
"e01" | "C" | "C_green" | "2023-01-15 05:00:00"

"e02" | "D" | "D_yellow"    | "2023-01-19 06:00:00"
"e02" | "K" | "K_white"     | "2023-01-19 06:00:00"
"e02" | "Z" | "Z_pink"      | "2023-01-19 06:00:00"

2. read files with rdd and dataframe

3. make transformations with SQL

4. create new columns

5. remove columns

6. use functions

7. with dataframe and sql, flatmap or explode


----------------------------------------------------------------

SQL

1. SQL Joins and Benn Diagrams
https://i.stack.imgur.com/UI25E.jpg
https://upload.wikimedia.org/wikipedia/commons/9/9d/SQL_Joins.svg



2. Window Functions
   select row with the max time_stamp of each day.

   table A
   | time_stamp | animal |
   |---|---|
   | 2023-05-20 5:00:00  | dog      |
   | 2023-05-20 6:00:00  | turttle  |
   | 2023-05-20 7:00:00  | cat      |

   | 2023-05-21 12:00:00  | fish    |
   | 2023-05-21 13:00:00  | hamster |
   | 2023-05-21 14:00:00  | rabit   |

RESULT
   | time_stamp | animal |
   |---|---|
   | 2023-05-20 7:00:00   | cat   |
   | 2023-05-21 14:00:00  | rabit |


3.

table A
| id  | color |
|---|---|
10  | green  |
20  | blue   |
30  | yellow |

table B
| id  | color |
|---|---|
20  | blue   |
30  | yellow |
40  | purple |

make a SQL query that return the unique like this

Result

| id  | color | message |
|---|---|
10  | green  | I am from table A |
40  | purple | I am from table B |

----------------------------------------------------------------
GCP

Dataproc
Apache Spark

Dataflow
Apache Beam
Streamming ETL with Dataflow, pub/sub and BigQuery

Composer
Apache Airflow
create simple ETL, from csv to postgresql
create Bid Data ETL, from csv to BigQuery, load a Dataflow job.

BigQuery
Pub/Sub

----------------------------------------------------------------