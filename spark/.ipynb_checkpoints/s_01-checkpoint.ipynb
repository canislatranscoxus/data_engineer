{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3e1d1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.11\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print( python_version() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee7d0266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/art/git/data_engineer/venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print( sys.executable )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "768aa721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=10, micro=11, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "print( sys.version_info )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0aba43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Jupyter core packages...\n",
      "IPython          : 8.14.0\n",
      "ipykernel        : 6.23.1\n",
      "ipywidgets       : 8.0.6\n",
      "jupyter_client   : 8.2.0\n",
      "jupyter_core     : 5.3.0\n",
      "jupyter_server   : 2.6.0\n",
      "jupyterlab       : not installed\n",
      "nbclient         : 0.8.0\n",
      "nbconvert        : 7.4.0\n",
      "nbformat         : 5.9.0\n",
      "notebook         : 6.5.4\n",
      "qtconsole        : 5.4.3\n",
      "traitlets        : 5.9.0\n"
     ]
    }
   ],
   "source": [
    "!jupyter --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44f7faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "#from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20471fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.0'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dacc10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc = SparkContext( 'local', 'my spark context' )\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName( 'frozen_friends' )\n",
    "         .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49ce33e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize( [ 'ana', 'elza', 'katy' ] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a75555d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04cbce30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ana'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c662b5a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ana', 'elza']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d62aba6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ana', 'elza', 'katy']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bring all the data\n",
    "\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6491f729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__annotations__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_computeFractionForSampleSize',\n",
       " '_defaultReducePartitions',\n",
       " '_id',\n",
       " '_is_barrier',\n",
       " '_is_protocol',\n",
       " '_jrdd',\n",
       " '_jrdd_deserializer',\n",
       " '_memory_limit',\n",
       " '_pickled',\n",
       " '_reserialize',\n",
       " '_to_java_object_rdd',\n",
       " 'aggregate',\n",
       " 'aggregateByKey',\n",
       " 'barrier',\n",
       " 'cache',\n",
       " 'cartesian',\n",
       " 'checkpoint',\n",
       " 'cleanShuffleDependencies',\n",
       " 'coalesce',\n",
       " 'cogroup',\n",
       " 'collect',\n",
       " 'collectAsMap',\n",
       " 'collectWithJobGroup',\n",
       " 'combineByKey',\n",
       " 'context',\n",
       " 'count',\n",
       " 'countApprox',\n",
       " 'countApproxDistinct',\n",
       " 'countByKey',\n",
       " 'countByValue',\n",
       " 'ctx',\n",
       " 'distinct',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'flatMap',\n",
       " 'flatMapValues',\n",
       " 'fold',\n",
       " 'foldByKey',\n",
       " 'foreach',\n",
       " 'foreachPartition',\n",
       " 'fullOuterJoin',\n",
       " 'getCheckpointFile',\n",
       " 'getNumPartitions',\n",
       " 'getResourceProfile',\n",
       " 'getStorageLevel',\n",
       " 'glom',\n",
       " 'groupBy',\n",
       " 'groupByKey',\n",
       " 'groupWith',\n",
       " 'has_resource_profile',\n",
       " 'histogram',\n",
       " 'id',\n",
       " 'intersection',\n",
       " 'isCheckpointed',\n",
       " 'isEmpty',\n",
       " 'isLocallyCheckpointed',\n",
       " 'is_cached',\n",
       " 'is_checkpointed',\n",
       " 'join',\n",
       " 'keyBy',\n",
       " 'keys',\n",
       " 'leftOuterJoin',\n",
       " 'localCheckpoint',\n",
       " 'lookup',\n",
       " 'map',\n",
       " 'mapPartitions',\n",
       " 'mapPartitionsWithIndex',\n",
       " 'mapPartitionsWithSplit',\n",
       " 'mapValues',\n",
       " 'max',\n",
       " 'mean',\n",
       " 'meanApprox',\n",
       " 'min',\n",
       " 'name',\n",
       " 'partitionBy',\n",
       " 'partitioner',\n",
       " 'persist',\n",
       " 'pipe',\n",
       " 'randomSplit',\n",
       " 'reduce',\n",
       " 'reduceByKey',\n",
       " 'reduceByKeyLocally',\n",
       " 'repartition',\n",
       " 'repartitionAndSortWithinPartitions',\n",
       " 'rightOuterJoin',\n",
       " 'sample',\n",
       " 'sampleByKey',\n",
       " 'sampleStdev',\n",
       " 'sampleVariance',\n",
       " 'saveAsHadoopDataset',\n",
       " 'saveAsHadoopFile',\n",
       " 'saveAsNewAPIHadoopDataset',\n",
       " 'saveAsNewAPIHadoopFile',\n",
       " 'saveAsPickleFile',\n",
       " 'saveAsSequenceFile',\n",
       " 'saveAsTextFile',\n",
       " 'setName',\n",
       " 'sortBy',\n",
       " 'sortByKey',\n",
       " 'stats',\n",
       " 'stdev',\n",
       " 'subtract',\n",
       " 'subtractByKey',\n",
       " 'sum',\n",
       " 'sumApprox',\n",
       " 'take',\n",
       " 'takeOrdered',\n",
       " 'takeSample',\n",
       " 'toDF',\n",
       " 'toDebugString',\n",
       " 'toLocalIterator',\n",
       " 'top',\n",
       " 'treeAggregate',\n",
       " 'treeReduce',\n",
       " 'union',\n",
       " 'unpersist',\n",
       " 'values',\n",
       " 'variance',\n",
       " 'withResources',\n",
       " 'zip',\n",
       " 'zipWithIndex',\n",
       " 'zipWithUniqueId']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir( rdd )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bd37b6",
   "metadata": {},
   "source": [
    "### convert RDD to dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c680179",
   "metadata": {},
   "source": [
    "spark can not infer the column data type from a rdd.\n",
    "\n",
    "So we map the each element in the rdd to a spark.sql.Row\n",
    "\n",
    "Next we can use the toDF( ) method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d93a7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = Row( 'friend_name' )\n",
    "df = rdd.map( row ).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e863976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|friend_name|\n",
      "+-----------+\n",
      "|        ana|\n",
      "|       elza|\n",
      "|       katy|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415e4cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce55893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffaa9772",
   "metadata": {},
   "source": [
    "### links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d73e2e",
   "metadata": {},
   "source": [
    "How to run pyspark from jupyter\n",
    "\n",
    "* https://medium.com/sicara/get-started-pyspark-jupyter-guide-tutorial-ae2fe84f594f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f37cb67",
   "metadata": {},
   "source": [
    "Course examples\n",
    "\n",
    "* https://github.com/rudrasingh21/Spark-By-Janani-Ravi/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f790fe3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
